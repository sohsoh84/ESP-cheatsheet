\section{Sample Statistics}
\subsection{Properties:}
Bias: $\expected[\hat{\theta}] - \theta$\\
Variance: $\text{Var}(\hat{\theta}) = \expected[(\hat{\theta} - \expected[\hat{\theta}])^2]$\\
Mean Squared Error: $\text{MSE}(\hat{\theta}) = \expected[(\hat{\theta} - \theta)^2] = 
\text{Var}(\hat{\theta}) + \text{Bias}^2$\\
Consistency: $lim_{n \to \infty} \hat{\theta} = \theta$\\
\subsection{Estimators:}
Sample Mean: $\bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i$\\
Sample Variance: $S^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2$\\
\subsection{Maximum Likelihood Estimation:}
$\hat{\theta}_{\text{MLE}} = \arg \max_{\theta} \prod_{i=1}^{n} f(X_i \| \theta) = \arg \max_{\theta} \sum_{i=1}^{n} \log f(X_i \| \theta)$\\
$\Rightarrow \frac{\partial_{\mathcal{L}}(\theta)}{\partial_\theta}$

\subsection{MMSE:}
linear MMSE: 
$h(a, b) = \expected[(Y - aX - b)^2]$\\
objective: minimize $h(a, b)$\\
solution: $a = \frac{\text{Cov}(X, Y)}{\text{Var}(X)} = \rho \frac{\sigma_Y}{\sigma_X}$, $b = \expected[Y] - a \expected[X]$

multiple MMSE:
% matrix form using covs 
\[
\begin{bmatrix}
\sigma_{X_1}^2 & \sigma_{X_1 X_2} \\
\sigma_{X_1 X_2} & \sigma_{X_2}^2
\end{bmatrix}
\begin{bmatrix}
a_1^* \\
a_2^*
\end{bmatrix}
=
\begin{bmatrix}
\mathrm{Cov}(X_1, Y) \\
\mathrm{Cov}(X_2, Y)
\end{bmatrix}
\]

\text{Covariance Matrix}

$
\hat{Y}(X_1, X_2) = a_1^* (X_1 - \mathbb{E}[X_1]) + a_2^* (X_2 - \mathbb{E}[X_2]) + \mathbb{E}[Y]
$

\subsection{Hypothesis Testing:}
Null Hypothesis: $H_0: \theta = \theta_0$\\
Alternative Hypothesis: $H_1: \theta \neq \theta_0$\\
Type I Error: Reject $H_0$ when it is true\\
Type II Error: Accept $H_0$ when it is false\\
P\_value: Probability of observing the data or more extreme data given $H_0$ is true\\
\subsection{Z-Test:}
$Z = \frac{\bar{X} - \mu}{\sigma / \sqrt{n}}$\\
$\Rightarrow \text{p-value} = 2 \cdot (1 - \Phi(|Z|))$\\ \\
Confidence Interval for Mean: $\bar{X} \pm Z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{n}}$\\

\subsection{T-test}:
Single Sample T-test:\\
$T = \frac{\bar{X} - \mu}{S / \sqrt{n}}$\\
$\Rightarrow \text{p-value} = 2 \cdot (1 - t_{n-1}(|T|))$\\
2-Sample T-test:\\
$T = \frac{\bar{X}_1 - \bar{X}_2}{\sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}}}$\\
$\Rightarrow \text{p-value} = 2 \cdot (1 - t_{n_1 + n_2 - 2}(|T|))$\\
2-Sample T-test (Unequal Variance):\\
$T = \frac{\bar{X}_1 - \bar{X}_2}{\sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}}}$\\
$\Rightarrow \text{p-value} = 2 \cdot (1 - t_{\nu}(|T|))$\\
where $\nu = \frac{\left(\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}\right)^2}{\frac{\left(\frac{S_1^2}{n_1}\right)^2}{n_1 - 1} + \frac{\left(\frac{S_2^2}{n_2}\right)^2}{n_2 - 1}}$\\

\subsection{Chi-Square Test:}
Goodness of Fit Test:\\
$\chi^2 = \sum_{i=1}^{k} \frac{(O_i - E_i)^2}{E_i}$\\
$\Rightarrow \text{p-value} = 1 - \chi^2_{k-1}(\chi^2)$\\

\subsection{Fisher's Exact Test:}
$H_0: \text{The two samples are from the same distribution}$\\
$H_1: \text{The two samples are from different distributions}$\\
$P = \frac{{n_1 \choose x} {n_2 \choose y}}{{n_1 + n_2 \choose x + y}}$\\
p\_value: getting the observed data or more extreme data\\

\subsection{Permutation Test:}
$H_0: \text{The two samples are from the same distribution}$\\
$H_1: \text{The two samples are from different distributions}$\\
consider an arbitrary test statistic $T$\\
permute the labels of the samples to get the distrbution of $T$ under $H_0$\\
then calculate the p-value as the probability of observing the data or more extreme data under $H_0$\\

\subsection{Beysian Inference:}
MAP: \\
$\hat{\theta}_{\text{MAP}} = \arg \max_{\theta} f(\theta \| X) = \arg \max_{\theta} f(X \| \theta) \cdot f(\theta)$\\

\subsection{Linear Regression:}
Simple Linear Regression: $Y = \beta_0 + \beta_1 X + \epsilon$\\
where $\epsilon \sim \mathcal{N}(0, \sigma^2)$\\
$\beta_1$ and $\beta_0$ can be estimated using the Maximum Likelihood Estimation\\
\[
\begin{array}{ll}
    \hat{\beta}_1 = \frac{\sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^{n} (X_i - \bar{X})^2} = \frac{\text{Cov}(X, Y)}{\text{Var}(X)}\\
    \hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X} \\
    \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n} (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i)^2
\end{array}
\]
